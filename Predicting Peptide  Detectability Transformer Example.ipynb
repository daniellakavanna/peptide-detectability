{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82bf8c28",
   "metadata": {},
   "source": [
    "## METHODOLOGY\n",
    "1. Subset Data 7-40 in length\n",
    "2. Balance the data by random sampling, this is because there is more of the undetected class, the detected class is the minority.\n",
    "3. Once dataset is balanced, integer encode the peptides. \n",
    "4. Test Train Split\n",
    "5. Create the model\n",
    "6. Fit the transformer\n",
    "7. Evaluate the model (TP,TN,F1,FP,FN/ PRECISION/RECALL TRADE OFF,  Plot Confusion matrix etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9767f3",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fec4ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "\n",
    "import sklearn.model_selection\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "from transformer import TokenAndPositionEmbedding, TransformerBlock\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from textwrap import wrap\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0809fe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This script is required to run the transformer network models and should be present in the same directory.\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\"\"\"\n",
    "## Implement multi head self attention as a Keras layer.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        query = self.separate_heads(\n",
    "            query, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key = self.separate_heads(\n",
    "            key, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(\n",
    "            value, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(\n",
    "            attention, perm=[0, 2, 1, 3]\n",
    "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(\n",
    "            concat_attention\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        return output\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Implement a Transformer block as a layer\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Implement embedding layer\n",
    "\n",
    "Two seperate embedding layers, one for tokens, one for token index (positions).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72a2004",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a9f095",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  pd.read_csv('updated_quant_missed_cleaved_peptides_excluded.csv') # read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b6f407",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94e57de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13add62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Detectability\"].sum() # number of detected peptides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a43acc",
   "metadata": {},
   "source": [
    "## Subset Peptide Sequences  between 7- 40 amino acids in length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13741708",
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_peptides = df[df['Detectability'] == 1] # detected peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efad30db",
   "metadata": {},
   "outputs": [],
   "source": [
    "undetected_peptides = df[df['Detectability'] == 0] # undetected peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a12e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "undetected_peptides.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cae6954",
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_peptides.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3433ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_peptides = detected_peptides.loc[(detected_peptides[\"Sequence\"].str.len()>=7) & \n",
    "                                          (detected_peptides[\"Sequence\"].str.len()<=40)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af69755",
   "metadata": {},
   "outputs": [],
   "source": [
    "undetected_peptides = undetected_peptides.loc[(undetected_peptides[\"Sequence\"].str.len()>=7) & \n",
    "                                              (undetected_peptides[\"Sequence\"].str.len()<=40)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee42c94",
   "metadata": {},
   "source": [
    "## Random Sampling to Balance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6308396",
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_peptides.shape # number of detected peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5c440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "undetected_peptides.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a648d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# undetected peptides balanced by number of rows of detected peptides..\n",
    "undetected_peptides_balanced = undetected_peptides.sample(n=detected_peptides.shape[0], \n",
    "                                                         random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8374aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "unused_undetected = undetected_peptides[~undetected_peptides[\"Sequence\"].isin\n",
    "                                        (undetected_peptides_balanced[\"Sequence\"])]\n",
    "unused_undetected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358f43ab",
   "metadata": {},
   "source": [
    "## drop unnessesary columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c562f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_peptides.drop(columns='Protein', axis=1, inplace=True)\n",
    "undetected_peptides_balanced.drop(columns='Protein', axis=1 ,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020191ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb810a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "undetected_peptides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357a3eb8",
   "metadata": {},
   "source": [
    "# Split into Train and Test Sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8c1b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detected peptides\n",
    "X_trainP, X_testP, y_trainP, y_testP = sklearn.model_selection.train_test_split(\n",
    "    detected_peptides, detected_peptides['Detectability'], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ab8c52",
   "metadata": {},
   "outputs": [],
   "source": [
    " #undetected peptides\n",
    "X_trainN, X_testN, y_trainN, y_testN = sklearn.model_selection.train_test_split(\n",
    "    undetected_peptides_balanced, undetected_peptides_balanced['Detectability'], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02963e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_trainP.shape)\n",
    "print(X_testP.shape)\n",
    "print('')\n",
    "print(X_trainN.shape)\n",
    "print(X_testN.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11691823",
   "metadata": {},
   "source": [
    "# split training into train and validation sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8de44fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detected peptides\n",
    "X_trainP, X_valP, y_trainP, y_valP = sklearn.model_selection.train_test_split(\n",
    "    X_trainP, y_trainP, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245a7d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# undetected peptides\n",
    "X_trainN, X_valN, y_trainN, y_valN = sklearn.model_selection.train_test_split(\n",
    "    X_trainN, y_trainN, test_size=0.3, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4441139",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_trainP.shape)\n",
    "print(X_valP.shape)\n",
    "print('')\n",
    "print(X_trainN.shape)\n",
    "print(X_valN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667cb307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create final training and validation sets \n",
    "X_train = pd.concat([X_trainP, X_trainN])\n",
    "X_val = pd.concat([X_valP] + [X_valN])\n",
    "y_train = pd.concat([pd.Series(y_trainP)] + [pd.Series(y_trainN)])\n",
    "y_val = pd.concat([pd.Series(y_valP)] + [pd.Series(y_valN)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b375dc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print('')\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3dfd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cafb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check validation set is not in train\n",
    "print(len(X_val[X_val[\"Sequence\"].isin(X_train[\"Sequence\"])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c54ad0c",
   "metadata": {},
   "source": [
    "# create final test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c934d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create final test set\n",
    "X_test = pd.concat([X_testP, X_testN])\n",
    "y_test = pd.concat([pd.Series(y_testP)] + [pd.Series(y_testN)])\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84ecd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check test is not in train or validation\n",
    "print(len(X_test[X_test[\"Sequence\"].isin(X_val[\"Sequence\"])]))\n",
    "print(len(X_test[X_test[\"Sequence\"].isin(X_train[\"Sequence\"])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8073835e",
   "metadata": {},
   "source": [
    "## Integer encode Raw  Peptide Sequences for Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603eae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_length = 40\n",
    "def convertPeptide(peptide_sequence, max_length): # set the maximum length of the peptide sequence\n",
    "    amino_acid = {'A': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'K': 9, 'L': 10, 'M': 11, 'N': 12, 'P': 13, 'Q': 14, 'R': 15, 'S': 16, 'T': 17,'U':18, 'V': 19, 'W': 20, 'Y': 21} # create a dictionary of amino acids\n",
    "    integer = [] # initialize an empty list\n",
    "    for i in range(max_length):\n",
    "        if i < len(peptide_sequence): #if the index is less than the length of the peptide sequence\n",
    "            integer.append(amino_acid[peptide_sequence[i]]) #append the value of the amino acid at the index to the integer\n",
    "        else: #if the index is greater than the length of the peptide sequence\n",
    "            integer.append(0) #append zero to the integer list\n",
    "    return np.array(integer)#return the integer as a numpy array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf3591b",
   "metadata": {},
   "source": [
    "### Shuffle the data inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8fdc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = shuffle(X_train, random_state=42).reset_index(drop=True)\n",
    "y_train = shuffle(y_train, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ed1b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = shuffle(X_val, random_state=42).reset_index(drop=True)\n",
    "y_val = shuffle(y_val, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4285dec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = shuffle(X_test, random_state=42).reset_index(drop=True)\n",
    "y_test = shuffle(y_test, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc9095b",
   "metadata": {},
   "source": [
    "## Separate out the features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592ab3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_peptide = X_train['Sequence'].apply(convertPeptide, args=(max_length,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e19dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_peptide # the training set is now a series of numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6358cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_quant= X_train.iloc[:,3] # get the quantitative features for the training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32713880",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_quant = X_train_quant.to_frame()    # convert the series to a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11737c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_quant['log'] = (X_train_quant['SMT'] + 1).transform(np.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850f285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_quant.drop(columns='SMT', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb38a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56900526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_quant = X_train_quant.to_numpy() # convert the series to a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fac6139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1365e4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ae9cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_quant = scaler.fit_transform(X_train_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a5a25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_quant = X_train_quant.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8f6114",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_peptide = X_val['Sequence'].apply(convertPeptide, args=(max_length,)) # get the encoded peptide sequences for the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113caf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_peptide # validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbf1487",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_quant = X_val.iloc[:,3] # get the quantitative values for the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ea802f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_quant = X_val_quant.to_frame() # convert the series to a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bb39cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val_quant['log'] = (X_val_quant['SMT'] +1).transform(np.log) # log the quantitative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9242fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val_quant.drop(columns='SMT', inplace=True) # drop the SMT column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc1d36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_quant # validation set with quantitative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71d4f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_quant = scaler.transform(X_val_quant).flatten() # scale the quantitative values for the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f0a7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_peptide = X_test['Sequence'].apply(convertPeptide, args=(max_length,)) # get the encoded peptide sequences for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722a80a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_peptide # test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7399058",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_quant = X_test.iloc[:,3]  # get the quantitative values for the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bb501d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_quant = X_test_quant.to_frame() # convert the series to a df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58eadb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_quant = scaler.transform(X_test_quant).flatten() # scale the quantitative values for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60681c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7490cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_quant['log'] = (X_test_quant['SMT'] +1).transform(np.log) # log the SMT values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b32f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_quant.drop(columns='SMT', inplace=True) # drop the SMT column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae899f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_quant = X_test_quant.to_numpy().flatten() # convert the series to a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb4be17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7cb5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the quantitative values to numpy arrays\n",
    "X_train_peptide = np.array(X_train_peptide.to_list()) # convert the series to a numpy array for the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc925981",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_peptide = np.array(X_val_peptide.to_list()) # convert the series to a numpy array for the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc4fe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_peptide = np.array(X_test_peptide.to_list()) # convert the series to a numpy array for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb61c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_peptide[0] # (the first peptide sequence, with the integer coversion applied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4013a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_peptide[9][:40] # take 40 amino acids of the 9th peptide, peptides < 40 are appended with 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5152a67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import EarlyStopping from keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b034270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(EarlyStopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18870c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss',  mode = 'min', patience=50, verbose=1) # set the early stopping parameters to monitor the validation loss and minimize the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca37fd0",
   "metadata": {},
   "source": [
    "### BUILD (REPORTER (rm) ITENSTIY QUANTIFICATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde44870",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32 # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "main_input = tf.keras.layers.Input(shape=(40,), name= 'sequence')\n",
    "# embed each peptide into a 40-dimensional vector\n",
    "embedding_layer = TokenAndPositionEmbedding(40, 22, embed_dim) # create a token and position embedding layer\n",
    "x = embedding_layer(main_input) # apply the token and position embedding layer to the main input\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim) # create a transformer block\n",
    "x = transformer_block(x) # apply the transformer block to the embedding layer\n",
    "\n",
    "y = tf.keras.layers.GlobalAveragePooling1D()(x) # apply global average pooling to the transformer block output\n",
    "auxiliary_output = tf.keras.layers.Dense(1, activation='sigmoid', name = 'aux_output')(y)\n",
    " # apply a dense layer to the output of the transformer block, y is the output of the global average pooling layer\n",
    " # create a dense layer with 1 output for the auxiliary output\n",
    "auxiliary_input = tf.keras.layers.Input(shape=(1,), name='reporter_ion_quant') # create an input layer for the auxiliary input\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# concatenate the output of the transformer block and the auxiliary input\n",
    "x = tf.keras.layers.concatenate([auxiliary_output, auxiliary_input]) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(x) # apply a dense layer to the concatenated output of the transformer block and the auxiliary input\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "\n",
    "main_output = tf.keras.layers.Dense(1, activation='sigmoid', name='main_output') (x) # apply a dense layer to the output of the dense layer, x is the output of the dense layer\n",
    "\n",
    "model = tf.keras.Model(inputs=[main_input, auxiliary_input], outputs=[main_output, auxiliary_output]) # create a model (defining it here because we need to specify the input and output layers)\n",
    "\n",
    "\n",
    "optimiser = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False) \n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer= optimiser, metrics=['accuracy'], loss_weights= [1.,0.2]) # compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8bf32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())\n",
    "keras.utils.plot_model(model, \"keras_model_sequence_SRI20date.png\", show_shapes=True, show_layer_names=True) #PLOT ARCHITECTURE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337c8c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.to_numpy() # convert the series to a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e8aee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = y_val.to_numpy() # convert the series to a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221c8740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val_quant = X_val_quant.to_numpy() # convert the series to a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661d6a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val_quant = X_val_quant.flatten() # flatten the numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8156978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_quant = X_train_quant.to_numpy().flatten() # convert the series to a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710faffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "history_model = model.fit([X_train_peptide, X_train_quant],  [y_train,y_train], validation_data= ([X_val_peptide, X_val_quant],[y_val,y_val]), callbacks=[early_stop] ,epochs=350, batch_size=128, verbose=2) # train the model with y_train and y_val as the labels\n",
    "print(\"\")\n",
    "print(\"Training time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e44a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40342ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss = pd.DataFrame(history_model.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391a981f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1ef5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss.to_csv(\"model_loss_sequence_min_max_NSRI_new_25date.csv\") # save the model loss to a csv file for the sequence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125ab51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model loss and training loss against the number of epochs\n",
    "# enlarge the figure size to make it easier to see\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(model_loss['loss'])\n",
    "plt.plot(model_loss['val_loss'])\n",
    "plt.title(' Transformer Training and Validation Loss (Sequence + NSRI)', fontsize=15)\n",
    "plt.ylabel('Loss', fontsize=13)\n",
    "plt.xlabel('Epochs', fontsize=13)\n",
    "plt.legend(['train loss', 'validation loss'], loc='upper right', fontsize=10)\n",
    "# x limit for the loss plot to 192\n",
    "plt.xlim(0,322)\n",
    "\n",
    "\n",
    "#plt.savefig('Transformer__Training_Val_loss_sequence_SMT_noise_07data128batches.png', dpi=300)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5436941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model training and validation accuracy against the number of epochs\n",
    "plt.plot(model_loss['main_output_accuracy'])\n",
    "plt.plot(model_loss['val_main_output_accuracy'])\n",
    "# enlarge the text size to make it easier to see\n",
    "plt.title(' Transformer Training and Validation Accuracy (Sequence + NSRI )', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.legend(['train accuracy', 'validation accuracy'], loc='lower right', fontsize=10)\n",
    "# x limit for the accuracy plot to 192\n",
    "plt.xlim(0, 322)\n",
    "#plt.savefig('Transformer_Training_Val_accuracy_sequence_SMT_noise_07.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90907cb2",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d2480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to a h5 file\n",
    "history_model.model.save('model_sequence_NSRI_min_max_25date') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b9e3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = model.evaluate([X_train_peptide, X_train_quant],[y_train,y_train]) # evaluate the model on the training data )\n",
    "print(\"\")\n",
    "print(\"Training results: \", train_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bfffa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8de80bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_results = model.evaluate([X_val_peptide,X_val_quant], [y_val, y_val]) # evaluate the model on the validation data\n",
    "print(\"\")\n",
    "print(\"Validation results: \", val_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a725be27",
   "metadata": {},
   "source": [
    "## Model Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f72fc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_prediction_df = pd.DataFrame({'Peptide': X_test['Sequence'], 'Detectability': y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eaa9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d8b0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions =model.predict([np.array([convertPeptide(x, max_length) for x in Model_prediction_df['Peptide']]), np.array(X_test_quant)],  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02469b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_prediction_df['Prediction'] = test_predictions[0].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f795550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_prediction_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d9b746",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d2f4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model predictions to a csv file\n",
    "Model_prediction_df.to_csv(\"model_predictions_sequence_min_max_NSRI_no_missed_cleaved_20date.csv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90648f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the model predictions from a csv file if needed\n",
    "Model_prediction_df = pd.read_csv(\"model_predictions_sequence_min_max_NSRI_no_missed_cleaved_20date.csv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf7f3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90f73b4",
   "metadata": {},
   "outputs": [],
   "source": [
    " colors = [\"#9f04ff\", \"#ffa304\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f18c570",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "plt.legend( loc='upper left', bbox_to_anchor=(1, 1))\n",
    "sns.histplot(data=Model_prediction_df, x=Model_prediction_df['Prediction'], hue=Model_prediction_df['Detectability'], palette= colors , stat='count', bins=25)\n",
    "ax.set_title('Transformer Sequence +  NSRI (by length)  ', fontsize=16)\n",
    "\n",
    "ax.set_xlabel('Prediction', fontsize=16)\n",
    "ax.set_ylabel('Frequency', fontsize=16)\n",
    "\n",
    "\n",
    "plt.axvline(x=Model_prediction_df['Detectability'].mean(), color='r')\n",
    "plt.axvline(x=Model_prediction_df['Prediction'].mean(), color='g', ls='--')\n",
    "plt.text(0.25,2300, ('Mean = {:.3f}'.format(Model_prediction_df['Prediction'].mean())), fontsize=13)\n",
    "# put text on the plot\n",
    "\n",
    "skew = round(((Model_prediction_df.loc[Model_prediction_df['Detectability'] == 1, 'Prediction'].mean() - Model_prediction_df.loc[Model_prediction_df['Detectability'] == 1, 'Prediction'].median()) * 3)/( Model_prediction_df.loc[Model_prediction_df['Detectability'] == 1, 'Prediction'].std()), 4)\n",
    "mean = round(Model_prediction_df.loc[Model_prediction_df['Detectability'] == 1, 'Prediction'].mean(), 4)\n",
    "median = round(Model_prediction_df.loc[Model_prediction_df['Detectability'] == 1, 'Prediction'].median(), 4)\n",
    "\n",
    "\n",
    "sd = round(Model_prediction_df.loc[Model_prediction_df['Detectability'] == 1, 'Prediction'].std(), 4)\n",
    "\n",
    "textstr = \"$\\overline {x}$\" + f\" = {mean} \\n $\\sigma$ = {sd} \\n median = {median} \\n Skew = {skew}\"\n",
    "props = dict(boxstyle='round', facecolor='yellow', alpha=0.2)\n",
    "plt.text(0.7,-1500, textstr, fontsize=13, bbox = props)\n",
    "\n",
    "\n",
    "skew = round(((Model_prediction_df.loc[Model_prediction_df['Detectability'] == 0, 'Prediction'].mean() - Model_prediction_df.loc[Model_prediction_df['Detectability'] == 0, 'Prediction'].median()) * 3)/( Model_prediction_df.loc[Model_prediction_df['Detectability'] == 0, 'Prediction'].std()), 4)\n",
    "mean = round(Model_prediction_df.loc[Model_prediction_df['Detectability'] == 0, 'Prediction'].mean(), 4)\n",
    "median = round(Model_prediction_df.loc[Model_prediction_df['Detectability'] == 0, 'Prediction'].median(), 4)\n",
    "\n",
    "# get the mode of the data where prediction is 1\n",
    "\n",
    "sd = round(Model_prediction_df.loc[Model_prediction_df['Detectability'] == 0, 'Prediction'].std(), 4)\n",
    "textstr = \"$\\overline {x}$\" + f\" = {mean} \\n $\\sigma$ = {sd} \\n median = {median} \\n Skew = {skew}\"\n",
    "props = dict(boxstyle='round', facecolor='violet', alpha=0.2)\n",
    "plt.text(0.1,-1500, textstr, fontsize=13, bbox = props)\n",
    "\n",
    "plt.savefig('Transformer_model_predictions_sequence_freq_min_max_scaled_SRI_noise_new_20date.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bb21ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = sklearn.metrics.confusion_matrix(y_test, np.rint(Model_prediction_df['Prediction']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfba9e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the classification report and plot confusion matrix\n",
    "\n",
    "print(sklearn.metrics.classification_report(y_test, np.rint(Model_prediction_df['Prediction'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f888348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, np.rint(Model_prediction_df['Prediction']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109a1571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "recall_score(y_test, round(Model_prediction_df['Prediction']), average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dc5bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "MCC = metrics.matthews_corrcoef(y_test, np.rint(Model_prediction_df['Prediction']))\n",
    "print(\"MCC:\",MCC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3d1abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test, np.rint(Model_prediction_df['Prediction']), average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1268fe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "precision_score(y_test, np.rint(Model_prediction_df['Prediction']), average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6d53ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9190c2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e95125",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(y_test)\n",
    "fig, ax = plt.subplots()\n",
    "#cm = confusion_matrix(y_test,  , labels=classes)\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap=plt.cm.Blues, cbar=False)\n",
    "ax.set(xlabel=\"Pred\", ylabel=\"True\", title=\"Confusion matrix\")\n",
    "ax.set_yticklabels(labels=classes, rotation=0)\n",
    "plt.savefig('Transformer_sequence__SRI_min_max_confusion_no_missed_cleaved_matrix_20date.png', dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcb25f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix_scores(df, peptide_col_index, true_col_index, pred_col_index): # df is the dataframe, peptide_col_index is the index of the peptide column, true_col_index is the index of the true column, pred_col_index is the index of the predicted column\n",
    "    scores_df = df.iloc[: , [peptide_col_index, true_col_index, pred_col_index]].copy() # create a copy of the dataframe\n",
    "    scores_df['pred_round'] = np.rint(scores_df.iloc[:, 2]).astype(int) # round to nearest integer\n",
    "    scores_df['Length'] = scores_df.iloc[:, 0].str.len() # get length of peptide\n",
    "    # count the number of missed cleavages, ignoring the last character of the peptide\n",
    "    scores_df['Missed Cleavages'] = scores_df.iloc[:, 0].str.slice(0, -1).str.count('K') + scores_df.iloc[:, 0].str.slice(0, -1).str.count('R') # get the number of missed cleavages\n",
    "    scores_df['pred_class'] = 0 # 0 = false, 1 = true\n",
    "    # set the pred_class to 1 if the predicted class is the same as the true class\n",
    "    scores_df.loc[(scores_df.iloc[:, 1] == 1) & (scores_df['pred_round'] == 1), 'pred_class'] = 'TP' # true positive # if the true class is 1 and the predicted class is 1\n",
    "    scores_df.loc[(scores_df.iloc[:, 1] == 1) & (scores_df['pred_round'] == 0), 'pred_class'] = 'FN' # false negative # if the true class is 1 and the predicted class is 0\n",
    "    scores_df.loc[(scores_df.iloc[:, 1] == 0) & (scores_df['pred_round'] == 0), 'pred_class'] = 'TN' # TN = true negative\n",
    "    scores_df.loc[(scores_df.iloc[:, 1] == 0) & (scores_df['pred_round'] == 1), 'pred_class'] = 'FP' # FP = false positive\n",
    "\n",
    "    scores_df['TP'] = 0 # initialize TP, FN, TN, FP columns\n",
    "    scores_df['FN'] = 0\n",
    "    scores_df['TN'] = 0\n",
    "    scores_df['FP'] = 0\n",
    "\n",
    "    scores_df.loc[(scores_df.iloc[:, 1] == 1) & (scores_df['pred_round'] == 1), 'TP'] = 1 # true positive\n",
    "    scores_df.loc[(scores_df.iloc[:, 1] == 1) & (scores_df['pred_round'] == 0), 'FN'] = 1\n",
    "    scores_df.loc[(scores_df.iloc[:, 1] == 0) & (scores_df['pred_round'] == 0), 'TN'] = 1 \n",
    "    scores_df.loc[(scores_df.iloc[:, 1] == 0) & (scores_df['pred_round'] == 1), 'FP'] = 1\n",
    "\n",
    "    # calculate the accuracy\n",
    "  #  scores_df['Accuracy'] = (scores_df['TP'] + scores_df['TN']) / (scores_df['TP'] + scores_df['TN'] + scores_df['FP'] + scores_df['FN'])\n",
    "\n",
    "\n",
    "    return scores_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52136a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = get_confusion_matrix_scores(Model_prediction_df, 0, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb2851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc_by_peptide_len(df):\n",
    "    columns = ['Length', 'TP', 'FN', 'TN', 'FP']\n",
    "    peptide_len_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "    for i in range(7, 41):\n",
    "        test = df[df['Peptide'].str.len() == i]\n",
    "\n",
    "        lenTP = test['TP'].sum()\n",
    "        lenFN = test['FN'].sum()\n",
    "        lenTN = test['TN'].sum()\n",
    "        lenFP = test['FP'].sum()\n",
    "\n",
    "        data = [i, lenTP, lenFN, lenTN, lenFP]\n",
    "        peptide_len_df.loc[i - 7] = data\n",
    "\n",
    "    peptide_len_df['Acc'] = (peptide_len_df['TP'] + peptide_len_df['TN']) / (peptide_len_df['TP'] + peptide_len_df['TN'] +\n",
    "                                                                             peptide_len_df['FP'] + peptide_len_df['FN'])\n",
    "\n",
    "    return peptide_len_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe87eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_by_len = get_acc_by_peptide_len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708c30c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_by_len # get the accuracy by peptide length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b621d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_by_len.to_csv('sequence_standardised_nsri_min_max_accuracy_no_missed_cleaved_by_length_for_transformerdate_22.csv', sep='\\t', index=False) # saved the accuracy by peptide length to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f613166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.figure(figsize=(12.5, 2.56))\n",
    "\n",
    "plt.plot(acc_by_len['Length'], acc_by_len['Acc'], color='tab:red', label='Sequence Feature')\n",
    "\n",
    "\n",
    "plt.title('Accuracy Trend over Peptide Length for the Transformer Sequence Model', fontsize=13)\n",
    "plt.xlabel('Peptide Length (aa)', fontsize=11)\n",
    "plt.ylabel('Accuracy', fontsize=11)\n",
    "plt.legend(loc='best', ncol=1)\n",
    "plt.savefig('Accuracy_Trend__missed_cleaved_SMT_over_length_for_the_Transformer_model_20.png', dpi=200, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90f3aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d789eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_df = acc_by_len.iloc[:, 1:5].apply(lambda x: x*100/sum(x), axis=1) # calculate the percentage of each \n",
    "stacked_df.index = range(7, 41) # set the index to the peptide length\n",
    "\n",
    "stacked_df.plot(kind='bar', stacked=True,\n",
    "          colormap=ListedColormap(sns.color_palette(\"tab20\", 9)),\n",
    "          figsize=(12,7))\n",
    "\n",
    "plt.title(\"Transformer Sequence +  NSRI (by length)  \", fontsize=18)\n",
    "plt.xlabel(\"Peptide Length (aa)\", fontsize=15)\n",
    "plt.ylabel(\"Prediction Classification (%)\", fontsize=15)\n",
    "plt.legend(fontsize=14)\n",
    "plt.xticks(rotation=360)\n",
    "plt.savefig('stacked_confusion_matrix_transformer_20date_sequence_NSRI_minmax_model_no_missed_cleaved22date.png', dpi=600, bbox_inches='tight')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e1081b051ffcf90783de786fe7e352be87c1f5042af9b0086632ae2d2fab680f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
